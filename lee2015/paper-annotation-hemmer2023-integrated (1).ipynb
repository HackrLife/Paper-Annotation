{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Annotations**\n",
    "\n",
    "# \"Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction\"\n",
    "\n",
    "**Hemmer et al. (2023)**\n",
    "\n",
    "*A Comprehensive Analysis of AI Delegation, Algorithmic Management, and Human Behavioural Response in Human-AI Teaming*\n",
    "\n",
    "---\n",
    "\n",
    "Critical Commentary, Theoretical Analysis,  \n",
    "Organisational Implications, and Research Gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Table of Contents**\n",
    "\n",
    "1. Executive Summary and Research Significance\n",
    "2. Paper Overview and Key Contributions\n",
    "3. Section-by-Section Annotations\n",
    "4. Methodological Deep Dive\n",
    "5. Theoretical Framework Analysis\n",
    "6. Critical Assessment and Limitations\n",
    "7. Implications for AI Governance Research\n",
    "8. Glossary of Technical Terms\n",
    "9. Suggested Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **1. Executive Summary and Research Significance**\n",
    "\n",
    "## 1.1 Why This Paper Matters\n",
    "\n",
    "Published at ACM IUI 2023, \"Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction\" represents a significant empirical contribution to understanding how humans respond when artificial intelligence systems take on managerial functions. While delegation algorithms have been proposed in the machine learning literature, this paper is among the first to rigorously test whether the psychological and behavioural assumptions underlying these algorithms hold in practice.\n",
    "\n",
    "The paper addresses a critical gap: prior work on AI delegation evaluated algorithms using either synthetically generated human predictions or predictions collected in annotation settings where participants were unaware of any AI involvement. Hemmer et al. ask the essential question \u2014 **does human behaviour change when people know they are being managed by an AI?** Their answer, surprisingly, is \"no\" \u2014 but this finding itself opens significant questions about the nature of algorithmic management.\n",
    "\n",
    "| **Historical Context** |\n",
    "|:----|\n",
    "| Before this paper, the dominant paradigm in human-AI collaboration research focused on **AI-assisted decision-making**, where humans receive AI recommendations and choose whether to accept them. This placed humans in the role of final decision-maker. **AI delegation inverts this relationship**: the AI decides which tasks humans should perform. This shift from \"AI as advisor\" to \"AI as manager\" has profound implications for workplace dynamics, yet had received minimal empirical attention with real human participants. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Paradigm Shift\n",
    "\n",
    "The paper's core contribution is demonstrating that AI delegation can improve both objective performance and subjective satisfaction \u2014 and crucially, that these improvements occur **regardless of whether humans are aware of the delegation**. This finding challenges the assumption that transparency about algorithmic management is necessary for positive outcomes.\n",
    "\n",
    "However, the finding also raises deeper questions. If humans perform better on capability-matched tasks whether or not they know an AI selected those tasks, what does this tell us about the mechanism? The authors identify **self-efficacy** as the mediating variable \u2014 humans feel more confident when given tasks matched to their abilities, and this confidence improves both performance and satisfaction.\n",
    "\n",
    "| **Critical Analysis** |\n",
    "|:----|\n",
    "| The title and framing emphasise the positive effects of AI delegation, but a more provocative reading is possible: **the paper demonstrates that algorithmic management can be effective without worker awareness or consent**. This has significant implications for workplace autonomy and informed consent that the paper acknowledges but does not deeply engage with. The gig economy applications cited (Uber, task platforms) have faced substantial criticism precisely for opaque algorithmic management. The finding that hidden delegation works equally well could be read as either reassuring (workers benefit regardless) or concerning (informed consent is unnecessary for extraction of improved performance). |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **2. Paper Overview and Key Contributions**\n",
    "\n",
    "## 2.1 Core Contributions\n",
    "\n",
    "The paper makes five distinct contributions to the literature on human-AI collaboration:\n",
    "\n",
    "1. **Empirical Validation of Delegation Algorithms:** Prior work proposed delegation algorithms (Raghu et al. 2019, Mozannar & Sontag 2020) but evaluated them with simulated or context-free human predictions. This paper provides the first rigorous test of whether these algorithms work when humans are aware of and potentially reactive to the delegation.\n",
    "\n",
    "2. **Mechanism Identification:** By measuring self-efficacy as a mediating variable, the paper moves beyond demonstrating \"what works\" to explaining \"why it works.\" This mechanistic insight has implications for designing future human-AI collaboration systems.\n",
    "\n",
    "3. **Hidden vs. Explicit Delegation Comparison:** The experimental design's inclusion of both aware and unaware delegation conditions allows direct comparison, resolving uncertainty about whether transparency matters for outcomes.\n",
    "\n",
    "4. **Task Satisfaction Measurement:** Most prior work on human-AI teaming focused exclusively on performance metrics. By including satisfaction measures, the paper addresses the human experience of algorithmic management.\n",
    "\n",
    "5. **Theoretical Integration:** The paper bridges organisational behaviour literature on supervisor-employee delegation with AI research, applying established constructs (self-efficacy, empowerment) to the novel context of algorithmic management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Author Team and Institutional Context\n",
    "\n",
    "The paper emerges from **Karlsruhe Institute of Technology (KIT)** in Germany, with collaboration from Ben-Gurion University of the Negev in Israel. KIT has established itself as a leading centre for research on AI-assisted decision-making and human-AI collaboration, with several authors (Hemmer, Schemmer, V\u00f6ssing, Satzger) publishing extensively in this space.\n",
    "\n",
    "| **Attribution Analysis** |\n",
    "|:----|\n",
    "| The author contributions span the full research pipeline: Hemmer and Schemmer on experimental design and analysis, Westphal bringing organisational behaviour expertise (particularly important for the self-efficacy framing), Vetter on implementation, and V\u00f6ssing and Satzger providing senior oversight. This interdisciplinary team structure \u2014 combining HCI, machine learning, and organisational psychology \u2014 is increasingly common and arguably necessary for rigorous human-AI collaboration research. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **3. Section-by-Section Annotations**\n",
    "\n",
    "## 3.1 Abstract Analysis\n",
    "\n",
    "The abstract establishes four key claims in 167 words:\n",
    "1. Prior work on AI delegation used simulated or context-independent human predictions\n",
    "2. This paper tests delegation with real participants who may or may not be aware of AI involvement\n",
    "3. Delegation improves both performance and satisfaction\n",
    "4. Self-efficacy mediates these effects\n",
    "\n",
    "| **Critical Analysis** |\n",
    "|:----|\n",
    "| The abstract makes the strong claim that findings \"provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces.\" This is a **significant interpretive leap** from an image classification task with crowdworkers to workplace management broadly. The hedging (\"initial evidence\", \"can be\") is appropriate, but the implication is clear: the authors see this as foundational work for algorithmic management systems. Readers should note this positioning. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Introduction (Section 1)\n",
    "\n",
    "The introduction situates AI delegation within the broader landscape of human-AI collaboration. The authors note that AI models now match or exceed human performance in specific domains, yet \"human predictions often remain more accurate for certain cases.\" This complementarity motivates delegation approaches that assign tasks to whichever party (human or AI) is more likely to succeed.\n",
    "\n",
    "**The key research questions are clearly stated:**\n",
    "\n",
    "> **RQ1:** How does AI delegation affect task performance compared to human and AI working alone?\n",
    ">\n",
    "> **RQ2:** How does AI delegation affect task satisfaction compared to human working alone?\n",
    ">\n",
    "> **RQ3:** What explains the effect of AI delegation on task performance and task satisfaction?\n",
    "\n",
    "| **Key Insight** |\n",
    "|:----|\n",
    "| The framing of RQ3 \u2014 asking \"what explains\" rather than \"does it work\" \u2014 reflects methodological sophistication. The authors anticipate finding positive effects and design the study to identify mechanisms. This *a priori* commitment to explanation, not just demonstration, strengthens the contribution. However, it also suggests the hypothesis space was narrowed before data collection; alternative mechanisms (e.g., reduced cognitive load, novelty effects) receive less attention. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Related Work (Section 2)\n",
    "\n",
    "The literature review distinguishes AI delegation from AI-assisted decision-making \u2014 the dominant paradigm where AI provides recommendations that humans accept or override. In AI-assisted settings, the human retains final decision authority, and research focuses on appropriate reliance (avoiding over- and under-reliance on AI recommendations).\n",
    "\n",
    "**AI delegation inverts this relationship:** the AI decides what humans should work on. The authors cite foundational delegation algorithms (Mozannar & Sontag 2020, Raghu et al. 2019, Wilder et al. 2020) that learn both AI and human capabilities to optimise task allocation. Crucially, these prior works \"assume that the behavior and perceptions of humans... remain unchanged whether or not an AI model delegates instances of a task.\"\n",
    "\n",
    "| **Critical Analysis** |\n",
    "|:----|\n",
    "| The related work section positions the paper well but **omits important adjacent literatures**. The algorithmic management literature (Lee et al. 2015, M\u00f6hlmann & Henfridsson 2019) \u2014 which documents worker experiences with platform-based algorithmic management \u2014 is cited only briefly. This literature has found predominantly **negative** worker perceptions of algorithmic management, creating tension with the positive findings here. The gig economy context receives acknowledgment but not deep engagement. Similarly, the automation and deskilling literature (Braverman 1974, Zuboff 1988) \u2014 which theorises how task allocation affects worker skill development \u2014 is absent. These omissions shape the paper's optimistic framing. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Theory Development and Hypotheses (Section 3)\n",
    "\n",
    "The theoretical framework draws on organisational behaviour research regarding supervisor-to-employee delegation. The authors argue that when delegation aligns with employee competencies, it results in \"more empowered, motivated, and higher-performing employees.\" They transfer this insight to AI delegation, hypothesising positive effects on performance (H1) and satisfaction (H2).\n",
    "\n",
    "The mechanistic hypothesis (H3, H4) posits **self-efficacy** as the mediating variable. Self-efficacy \u2014 \"a person's belief in one's own ability to complete a task successfully\" (Bandura 1977) \u2014 is a well-established construct in organisational psychology. The authors argue that capability-matched task delegation should increase self-efficacy, which in turn improves outcomes.\n",
    "\n",
    "| **Theoretical Integration** |\n",
    "|:----|\n",
    "| The choice to draw on organisational behaviour literature rather than pure AI/HCI research is strategic and productive. It provides established constructs (self-efficacy, empowerment) with validated measurement instruments, grounds the work in a broader theoretical tradition, and makes the findings legible to management scholars. However, this framing also imports assumptions \u2014 particularly that **AI-to-human delegation functions analogously to supervisor-to-employee delegation**. Whether this analogy holds is an empirical question the paper assumes rather than tests. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Methodology (Section 4)\n",
    "\n",
    "### 3.5.1 Data and Task Selection\n",
    "\n",
    "The study uses a modified ImageNet subset (Steyvers et al. 2022) with 1,200 images across 16 classes, distorted with phase noise to create difficulty for both humans and AI. This task choice is deliberate: it requires no specialised expertise (ensuring participant recruitment feasibility) while allowing similar human and AI performance levels (ensuring meaningful delegation decisions).\n",
    "\n",
    "| **Critical Analysis: Task Ecological Validity** |\n",
    "|:----|\n",
    "| The image classification task, while experimentally clean, **differs substantially from real-world algorithmic management contexts**. In gig economy platforms, tasks often have: (1) variable difficulty that may not be estimable a priori, (2) temporal dependencies between tasks, (3) worker learning and skill development over time, (4) real financial stakes, (5) social/status implications of task assignment. The distorted ImageNet setup eliminates these complexities, enabling causal inference but limiting generalisability. The authors acknowledge this in limitations but the experimental design prioritises internal over external validity. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 AI Model Architecture\n",
    "\n",
    "The delegation algorithm implements Raghu et al.'s (2019) approach: a **DenseNet-161** classifier estimates AI confidence while a parallel \"human error model\" estimates human confidence from training data. Instances are delegated to humans when estimated human confidence exceeds AI confidence.\n",
    "\n",
    "The model was trained on 60% of the data with 20% validation and 20% test sets. Fine-tuning used SGD with learning rate 10\u207b\u2074, weight decay 5\u00d710\u207b\u2074, cosine annealing schedule, batch size 16, and early stopping on validation loss.\n",
    "\n",
    "| **Technical Note** |\n",
    "|:----|\n",
    "| The delegation algorithm's effectiveness depends critically on the quality of human confidence estimates. These estimates are derived from **historical human predictions** in the training data \u2014 predictions collected without any AI involvement. The paper assumes these historical predictions generalise to new participants and to participants who know about AI delegation. This assumption is testable but not tested; it would require comparing predicted vs. actual human confidence in the experimental conditions. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Experimental Design\n",
    "\n",
    "The study employed a **between-subjects design** with three conditions:\n",
    "\n",
    "1. **Delegation Condition (n=66):** Participants classified 20 images randomly drawn from instances the AI model selected for delegation. They were explicitly told \"the AI will decide for each image whether to label it alone or to pass it on to you for labeling.\" Additionally, 5 images were shown with messages indicating the AI had already classified them.\n",
    "\n",
    "2. **Hidden Delegation Condition (n=67):** Participants classified the same set of delegated images but without any mention of AI involvement. Instructions simply stated \"Just as previously, you will decide on the label for each image.\"\n",
    "\n",
    "3. **Human-Alone Control (n=63):** Participants classified 20 images randomly drawn from the full test set, with no AI involvement mentioned.\n",
    "\n",
    "| **Design Analysis** |\n",
    "|:----|\n",
    "| The three-condition design is well-constructed for the primary research questions but has limitations. The delegation and hidden delegation conditions use **the same image subset** (those selected for human delegation), while the human-alone condition uses random images. This means condition differences conflate two factors: (1) task difficulty/suitability and (2) awareness of AI involvement. The authors address this by noting that the key comparison for awareness effects is delegation vs. hidden delegation, which use identical stimuli. However, the human-alone condition serves as baseline for both performance and mechanism analyses, and its different stimulus set is a potential confound. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **4. Methodological Deep Dive**\n",
    "\n",
    "## 4.1 Participant Recruitment and Sample\n",
    "\n",
    "196 participants were recruited via **Prolific Academic** after exclusions for failed attention/manipulation checks (13 participants) and missing data (1 participant). The sample had mean age 39.43 years (SD=13.13) and was 58.67% female. Participants received $1.5 for approximately 10 minutes of work, yielding an effective hourly rate of $9/hour.\n",
    "\n",
    "Sample size was determined a priori using G*Power, targeting detection of small effects (f\u00b2=0.10) with 0.95 power in a three-group regression with three predictors. This required 176 participants; 210 were recruited anticipating attrition.\n",
    "\n",
    "| **Methodological Strength** |\n",
    "|:----|\n",
    "| Pre-registered power analysis with a priori sample size determination is best practice that many HCI studies omit. The conservative small effect size assumption is appropriate for a novel manipulation. However, the power analysis assumes the regression model specified; the actual analyses include seven control variables (not three), which affects power and multiple comparison concerns. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Measurement Instruments\n",
    "\n",
    "**Task Performance:** Measured as classification accuracy (percentage correct out of 20 images). This is an objective measure with clear interpretation.\n",
    "\n",
    "**Task Satisfaction:** Three items adapted from Hofmann & Strickland (1995) on 5-point Likert scales: overall satisfaction with performance, satisfaction with learning, enjoyment of the task. Cronbach's \u03b1 = 0.73 (acceptable but not strong internal consistency).\n",
    "\n",
    "**Self-Efficacy:** Three items adapted from Spreitzer (1995) on 7-point Likert scales: confidence in ability, mastery of necessary skills, self-assurance about capabilities. Cronbach's \u03b1 = 0.89 (good internal consistency).\n",
    "\n",
    "**Control Variables:** Task experience, algorithm attitude (5-point), algorithm use frequency (5-point), cognitive ability for visual tasks (4 items, 7-point, \u03b1=0.86), education, age, gender.\n",
    "\n",
    "| **Measurement Validity** |\n",
    "|:----|\n",
    "| The self-efficacy and task satisfaction measures are adapted from validated scales, providing construct validity. However, \"task satisfaction\" in a **10-minute crowdworking context differs substantially from \"job satisfaction\"** in ongoing employment relationships. The satisfaction measure captures immediate reaction to a brief task, not the sustained satisfaction relevant to workplace algorithmic management. Additionally, the cognitive ability measure is self-reported rather than objectively assessed, introducing potential bias. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Results Summary\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Outcome': ['Task Performance (%)', 'Task Satisfaction (1-5)', 'Self-Efficacy (1-7)'],\n",
    "    'Delegation': ['84.51 (11.24)', '3.65 (0.66)', '5.29 (0.96)'],\n",
    "    'Hidden Delegation': ['83.73 (12.29)', '3.62 (0.73)', '5.37 (1.05)'],\n",
    "    'Human-Alone': ['67.13 (13.11)', '3.35 (0.70)', '4.63 (1.25)'],\n",
    "    'Del. vs Human-Alone': ['p < 0.001', 'p < 0.01', 'p < 0.001'],\n",
    "    'Del. vs Hidden': ['n.s.', 'n.s.', 'n.s.']\n",
    "})\n",
    "\n",
    "print(\"KEY FINDINGS: Mean (SD) by Condition\")\n",
    "print(\"=\" * 80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"\\nNote: n.s. = not significant; all delegation vs. human-alone comparisons significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Main Results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "conditions = ['Delegation', 'Hidden\\nDelegation', 'Human\\nAlone']\n",
    "colors = ['#27AE60', '#3498DB', '#E74C3C']\n",
    "\n",
    "# Task Performance\n",
    "perf = [84.51, 83.73, 67.13]\n",
    "perf_err = [11.24/np.sqrt(66), 12.29/np.sqrt(67), 13.11/np.sqrt(63)]\n",
    "axes[0].bar(conditions, perf, yerr=perf_err, color=colors, alpha=0.8, capsize=5)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Task Performance', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].axhline(y=75.83, color='gray', linestyle='--', label='AI alone (75.83%)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Task Satisfaction\n",
    "sat = [3.65, 3.62, 3.35]\n",
    "sat_err = [0.66/np.sqrt(66), 0.73/np.sqrt(67), 0.70/np.sqrt(63)]\n",
    "axes[1].bar(conditions, sat, yerr=sat_err, color=colors, alpha=0.8, capsize=5)\n",
    "axes[1].set_ylabel('Score (1-5)', fontsize=12)\n",
    "axes[1].set_title('Task Satisfaction', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylim(1, 5)\n",
    "\n",
    "# Self-Efficacy\n",
    "eff = [5.29, 5.37, 4.63]\n",
    "eff_err = [0.96/np.sqrt(66), 1.05/np.sqrt(67), 1.25/np.sqrt(63)]\n",
    "axes[2].bar(conditions, eff, yerr=eff_err, color=colors, alpha=0.8, capsize=5)\n",
    "axes[2].set_ylabel('Score (1-7)', fontsize=12)\n",
    "axes[2].set_title('Self-Efficacy (Mediator)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylim(1, 7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Key Experimental Results: Hemmer et al. (2023)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n*** Delegation conditions significantly outperform Human-Alone (p < 0.01)\")\n",
    "print(\"*** No significant difference between Delegation and Hidden Delegation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **5. Theoretical Framework Analysis**\n",
    "\n",
    "## 5.1 Self-Efficacy as Mediating Mechanism\n",
    "\n",
    "The paper's core theoretical contribution is identifying **self-efficacy** as the mechanism linking AI delegation to improved outcomes. The mediation model shows that:\n",
    "\n",
    "- Delegation increases self-efficacy (\u03b2=0.75-0.80, p<0.001)\n",
    "- Increased self-efficacy improves both performance (\u03b2=0.62, p<0.001) and satisfaction (\u03b2=0.39, p<0.001)\n",
    "\n",
    "The self-efficacy mediation **fully accounts** for the effect on satisfaction (direct effect becomes non-significant) but only **partially mediates** the performance effect (direct effect remains significant). This suggests performance improvements operate through multiple channels, with self-efficacy being one important pathway.\n",
    "\n",
    "| **Theoretical Interpretation** |\n",
    "|:----|\n",
    "| The finding that self-efficacy fully mediates satisfaction effects is theoretically coherent: feeling capable makes work more enjoyable. The partial mediation for performance suggests **additional mechanisms**. One plausible candidate is simply task difficulty \u2014 delegated images may be objectively easier for humans regardless of subjective confidence. The paper does not report objective difficulty measures for delegated vs. non-delegated images, making it impossible to distinguish capability-matching from difficulty-selection. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Mediation Model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "# Boxes\n",
    "boxes = {\n",
    "    'AI Delegation': (1, 3, 2, 1),\n",
    "    'Self-Efficacy': (4.5, 5, 2, 1),\n",
    "    'Task\\nPerformance': (8, 4, 1.5, 1),\n",
    "    'Task\\nSatisfaction': (8, 1.5, 1.5, 1)\n",
    "}\n",
    "\n",
    "for label, (x, y, w, h) in boxes.items():\n",
    "    rect = mpatches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.05\",\n",
    "                                    facecolor='#D5E8F0', edgecolor='#2C3E50', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2, label, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Arrows with labels\n",
    "ax.annotate('', xy=(4.5, 5.5), xytext=(3, 3.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27AE60', lw=2))\n",
    "ax.text(3.5, 4.8, '\u03b2=0.75-0.80***', fontsize=10, color='#27AE60')\n",
    "\n",
    "ax.annotate('', xy=(8, 4.5), xytext=(6.5, 5.3),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27AE60', lw=2))\n",
    "ax.text(7, 5.2, '\u03b2=0.62***', fontsize=10, color='#27AE60')\n",
    "\n",
    "ax.annotate('', xy=(8, 2), xytext=(6.5, 5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#27AE60', lw=2))\n",
    "ax.text(6.8, 3.3, '\u03b2=0.39***', fontsize=10, color='#27AE60')\n",
    "\n",
    "# Direct effects\n",
    "ax.annotate('', xy=(8, 4.2), xytext=(3, 3.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='#E74C3C', lw=2, linestyle='--'))\n",
    "ax.text(5, 3.2, 'Direct: \u03b2=2.9-3.0***\\n(partial mediation)', fontsize=9, color='#E74C3C')\n",
    "\n",
    "ax.annotate('', xy=(8, 1.8), xytext=(3, 3.2),\n",
    "            arrowprops=dict(arrowstyle='->', color='#3498DB', lw=2, linestyle='--'))\n",
    "ax.text(4.5, 2.2, 'Direct: n.s.\\n(full mediation)', fontsize=9, color='#3498DB')\n",
    "\n",
    "ax.set_title('Mediation Model: Self-Efficacy as Mechanism', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"*** p < 0.001\")\n",
    "print(\"\\nKey finding: Self-efficacy FULLY mediates satisfaction effect,\")\n",
    "print(\"but only PARTIALLY mediates performance effect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Organisational Behaviour Framing\n",
    "\n",
    "The paper frames AI delegation as analogous to supervisor-to-employee delegation, drawing on management literature (Schriesheim et al. 1998, Leana 1986, 1987). This framing provides theoretical grounding but may **obscure important differences**:\n",
    "\n",
    "Human supervisors have:\n",
    "- Social relationships with employees\n",
    "- Ability to provide feedback and mentorship\n",
    "- Capacity to explain delegation rationale when asked\n",
    "- Adaptation based on observation\n",
    "- Accountability to the same organisation\n",
    "\n",
    "AI delegation systems **lack** these characteristics \u2014 they optimise based on historical patterns without social context or ongoing relationship.\n",
    "\n",
    "| **Theoretical Limitation** |\n",
    "|:----|\n",
    "| The analogy between human and AI delegation may not hold under conditions that **activate the differences**. In the experimental setup, participants completed a brief task with no ongoing relationship, feedback, or stakes \u2014 conditions that minimise the relevance of supervisor-employee relationship dynamics. In sustained workplace contexts, the absence of relationship, explanation, and accountability may become salient and undermine the positive effects observed here. The Uber driver studies (M\u00f6hlmann & Henfridsson 2019) suggest this is indeed the case. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Missing Theoretical Lenses\n",
    "\n",
    "Several theoretical frameworks relevant to AI delegation receive minimal attention:\n",
    "\n",
    "**Automation and Deskilling:** Braverman's (1974) labour process theory and subsequent work on technological deskilling predicts that algorithmic task allocation may reduce worker skill development by limiting exposure to challenging tasks. The paper's short-term focus cannot assess this.\n",
    "\n",
    "**Principal-Agent Theory:** The AI delegation system creates a novel principal-agent relationship where the AI acts as agent for organisational principals in managing human workers. Standard agency concerns (information asymmetry, goal misalignment, monitoring costs) apply but are not analysed.\n",
    "\n",
    "**Street-Level Bureaucracy:** Lipsky's (1980, 2010) framework on discretion and adaptation in implementation contexts could illuminate how workers might game or resist algorithmic management \u2014 a possibility the paper acknowledges but does not theorise.\n",
    "\n",
    "**Sociotechnical Systems:** The paper treats the AI delegation system as a technical intervention without analysing how it would integrate with existing workplace social and technical systems (Trist & Bamforth 1951, Bostrom & Heinen 1977)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **6. Critical Assessment and Limitations**\n",
    "\n",
    "## 6.1 What the Paper Gets Right\n",
    "\n",
    "\u2713 **Rigorous Experimental Design:** The three-condition between-subjects design with pre-registered sample size provides strong causal inference for the primary research questions.\n",
    "\n",
    "\u2713 **Mechanism Identification:** Moving beyond \"does it work?\" to \"why does it work?\" represents methodological maturity.\n",
    "\n",
    "\u2713 **Interdisciplinary Integration:** Drawing on organisational behaviour literature for constructs and measures strengthens the theoretical grounding.\n",
    "\n",
    "\u2713 **Appropriate Hedging:** The paper generally avoids overclaiming, acknowledging limitations and calling for future research.\n",
    "\n",
    "## 6.2 Significant Limitations\n",
    "\n",
    "| **\u26a0\ufe0f Limitation 1: Task Ecological Validity** |\n",
    "|:----|\n",
    "| The distorted image classification task differs fundamentally from real-world algorithmic management contexts. It involves no learning, no interdependence, no career stakes, no social comparison, and no ongoing relationship with the algorithmic system. Findings may not transfer to contexts where these factors matter \u2014 which is most workplace contexts. |\n",
    "\n",
    "| **\u26a0\ufe0f Limitation 2: Participant Population** |\n",
    "|:----|\n",
    "| Prolific crowdworkers are not representative of workers subject to algorithmic management in practice. They self-select into short tasks, may have different attitudes toward algorithmic systems, and lack organisational embeddedness. |\n",
    "\n",
    "| **\u26a0\ufe0f Limitation 3: Short Time Horizon** |\n",
    "|:----|\n",
    "| The 10-minute task duration cannot capture effects that emerge over time: deskilling, learned helplessness, adaptation of strategies to game the algorithm, erosion of initial positive effects, or cumulative satisfaction/dissatisfaction. |\n",
    "\n",
    "| **\u26a0\ufe0f Limitation 4: Organisational Context Abstraction** |\n",
    "|:----|\n",
    "| The experimental design eliminates all organisational context: no managers, no coworkers, no organisational culture, no career incentives, no power dynamics. This enables clean causal inference but severely limits applicability to real organisations. |\n",
    "\n",
    "| **\u26a0\ufe0f Limitation 5: Incentive and Gaming Dynamics** |\n",
    "|:----|\n",
    "| The paper does not consider strategic behaviour. In ongoing algorithmic management, workers might learn to manipulate capability signals, potentially corrupting delegation algorithm accuracy. This Goodhart's Law dynamic is central to algorithmic management critiques but unexamined here. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **7. Implications for AI Governance Research**\n",
    "\n",
    "## 7.1 Contribution to Thesis Argument\n",
    "\n",
    "This paper provides important evidence for the thesis that **AI governance literature treats organisations as neutral transmission mechanisms** for technical solutions. The paper demonstrates positive effects of AI delegation in a context that has been deliberately purged of organisational dynamics \u2014 and the authors explicitly recommend that \"managers could consider applying AI delegation to yield higher levels of performance and employee satisfaction\" based on these findings.\n",
    "\n",
    "This represents the pattern identified across the AI governance literature: technically rigorous work that validates solutions under controlled conditions, acknowledges organisational factors as \"limitations,\" but nonetheless draws implications for organisational practice. **The gap between laboratory validation and organisational implementation is acknowledged but not bridged.**\n",
    "\n",
    "| **Pattern Confirmation** |\n",
    "|:----|\n",
    "| The paper confirms a recurring pattern across the human-AI collaboration literature: **ALGORITHMIC MANAGEMENT** is framed as a **TECHNICAL/BEHAVIOURAL problem** (can we design algorithms that effectively match tasks to capabilities?) while **ORGANISATIONAL/INSTITUTIONAL factors** (power dynamics, resistance, gaming, deskilling) are acknowledged as limitations but not integrated into the research design or theoretical framework. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Conditions Analysis\n",
    "print(\"=\"*70)\n",
    "print(\"BOUNDARY CONDITIONS FOR CLAIMS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\u2713 CLAIMS HOLD WHEN:\")\n",
    "holds = [\n",
    "    \"Tasks are independent and do not require coordination\",\n",
    "    \"Workers have no intrinsic interest in task variety or challenge\",\n",
    "    \"Performance is easily measurable and aligned with organisational goals\",\n",
    "    \"Time horizons are short (no skill development concerns)\",\n",
    "    \"Workers have no power to resist or exit the system\",\n",
    "    \"No regulatory requirements for human oversight or explanation\"\n",
    "]\n",
    "for h in holds:\n",
    "    print(f\"  \u2022 {h}\")\n",
    "\n",
    "print(\"\\n\u2717 CLAIMS LIKELY BREAK WHEN:\")\n",
    "breaks = [\n",
    "    \"Middle managers perceive AI delegation as threatening their role\",\n",
    "    \"Workers value autonomy in task selection as part of professional identity\",\n",
    "    \"Tasks require tacit knowledge that develops through diverse experience\",\n",
    "    \"Workers can strategically manipulate capability signals\",\n",
    "    \"Organisational culture emphasises human judgement and discretion\",\n",
    "    \"Regulatory frameworks require transparency in automated decision-making\"\n",
    "]\n",
    "for b in breaks:\n",
    "    print(f\"  \u2022 {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Future Research Directions\n",
    "\n",
    "For the AI governance research agenda, this paper suggests several productive directions:\n",
    "\n",
    "1. **Field Studies of Algorithmic Management:** Laboratory findings need validation in real organisational contexts with ongoing relationships, career stakes, and social dynamics. Ethnographic and longitudinal approaches would complement experimental methods.\n",
    "\n",
    "2. **Gaming and Adaptation Dynamics:** How do workers learn about and respond to delegation algorithms over time? Do initial positive effects persist or erode? This requires studying deployed systems over extended periods.\n",
    "\n",
    "3. **Organisational Implementation Barriers:** What factors determine whether organisations successfully implement AI delegation? Principal-agent theory, institutional theory, and change management frameworks could inform this research.\n",
    "\n",
    "4. **Accountability and Governance Structures:** How should responsibility be allocated when AI delegation leads to errors? What oversight mechanisms are appropriate? This requires engaging legal and organisational governance literatures.\n",
    "\n",
    "5. **Distributional and Equity Effects:** Do AI delegation systems systematically advantage or disadvantage particular worker groups? Fairness in algorithmic management requires examining heterogeneous effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **8. Glossary of Technical Terms**\n",
    "\n",
    "**AI Delegation:** A form of human-AI collaboration where the AI system decides which task instances to handle itself and which to assign to human workers, based on estimated capabilities of each party.\n",
    "\n",
    "**Algorithmic Management:** The use of algorithms to perform managerial functions such as task allocation, performance monitoring, and evaluation. Common in gig economy platforms.\n",
    "\n",
    "**Confidence Calibration:** The alignment between a model's expressed confidence and its actual accuracy. A well-calibrated model that says it's 80% confident should be correct 80% of the time.\n",
    "\n",
    "**DenseNet:** A convolutional neural network architecture where each layer connects to all subsequent layers, enabling feature reuse and gradient flow.\n",
    "\n",
    "**Human Error Model:** A model that predicts whether humans will correctly classify specific instances, trained on historical human prediction data.\n",
    "\n",
    "**Mediation Analysis:** Statistical technique to test whether the effect of an independent variable on a dependent variable operates through an intermediate (mediating) variable.\n",
    "\n",
    "**PROCESS Macro:** A computational tool for mediation, moderation, and conditional process analysis developed by Andrew Hayes.\n",
    "\n",
    "**Prolific Academic:** An online platform for recruiting research participants, generally considered higher quality than Amazon Mechanical Turk for academic research.\n",
    "\n",
    "**Self-Efficacy:** A person's belief in their capability to execute behaviours necessary to produce specific outcomes (Bandura 1977). Distinct from actual capability.\n",
    "\n",
    "**Task Satisfaction:** An individual's affective response to their work on a specific task, including enjoyment and sense of accomplishment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **9. Suggested Further Reading**\n",
    "\n",
    "## 9.1 Foundational Papers on AI Delegation\n",
    "\n",
    "- **Raghu et al. (2019)** \u2014 \"The Algorithmic Automation Problem: Prediction, Triage, and Human Effort\" \u2014 The delegation algorithm implemented in this paper.\n",
    "- **Mozannar & Sontag (2020)** \u2014 \"Consistent Estimators for Learning to Defer to an Expert\" \u2014 Theoretical foundations for human-AI delegation.\n",
    "- **Wilder, Horvitz & Kamar (2020)** \u2014 \"Learning to Complement Humans\" \u2014 Optimising human-AI team performance through capability-aware delegation.\n",
    "\n",
    "## 9.2 Algorithmic Management Literature\n",
    "\n",
    "- **Lee et al. (2015)** \u2014 \"Working with Machines: The Impact of Algorithmic and Data-Driven Management on Human Workers\" \u2014 Foundational study on worker experiences with algorithmic management.\n",
    "- **M\u00f6hlmann & Henfridsson (2019)** \u2014 \"What People Hate About Being Managed by Algorithms\" \u2014 Qualitative study of Uber driver perceptions.\n",
    "- **Benlian et al. (2022)** \u2014 \"Algorithmic Management: Bright and Dark Sides\" \u2014 Comprehensive review of algorithmic management research.\n",
    "\n",
    "## 9.3 Self-Efficacy and Organisational Behaviour\n",
    "\n",
    "- **Bandura (1977)** \u2014 \"Self-Efficacy: Toward a Unifying Theory of Behavioral Change\" \u2014 The foundational paper on self-efficacy.\n",
    "- **Spreitzer (1995)** \u2014 \"Psychological Empowerment in the Workplace\" \u2014 Source of the self-efficacy measurement scale used.\n",
    "- **Schriesheim, Neider & Scandura (1998)** \u2014 \"Delegation and Leader-Member Exchange\" \u2014 Research on supervisor delegation.\n",
    "\n",
    "## 9.4 Critical Perspectives\n",
    "\n",
    "- **Braverman (1974)** \u2014 \"Labor and Monopoly Capital\" \u2014 Classic analysis of technological deskilling.\n",
    "- **Zuboff (1988)** \u2014 \"In the Age of the Smart Machine\" \u2014 Foundational work on information technology and work transformation.\n",
    "- **Lipsky (1980, 2010)** \u2014 \"Street-Level Bureaucracy\" \u2014 Framework on discretion and adaptation in implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*\u2014 End of Annotations \u2014*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "# **PART 2: TEMPLATED PARAMETER MAPPING**\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Annotation: Human-AI Collaboration\n",
    "## The Effect of AI Delegation on Human Task Performance and Task Satisfaction\n",
    "\n",
    "**Hemmer et al. (2023) | ACM IUI**\n",
    "\n",
    "---\n",
    "\n",
    "| Field | Value |\n",
    "|-------|-------|\n",
    "| **Annotation Date** | January 22, 2026 |\n",
    "| **Analysis Mode** | Full Analysis |\n",
    "| **Annotator** | PhD Literature Review Framework |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Triage\n",
    "\n",
    "| Criterion | Assessment |\n",
    "|-----------|------------|\n",
    "| **Paper Type** | \ud83d\udfe2 **Core** |\n",
    "| **Relevance Score** | 5/5 |\n",
    "| **Bucket** | Human-Machine Interaction |\n",
    "| **Rationale** | Directly addresses human behaviour in AI delegation contexts, examines self-efficacy as mediating mechanism, and tests assumptions about human-AI collaboration in organizational settings. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Paper Identity & Positioning\n",
    "\n",
    "```\n",
    "Title:    Human-AI Collaboration: The Effect of AI Delegation on Human \n",
    "          Task Performance and Task Satisfaction\n",
    "Authors:  Patrick Hemmer, Monika Westphal, Max Schemmer, Sebastian Vetter,\n",
    "          Michael V\u00f6ssing, Gerhard Satzger\n",
    "Year:     2023\n",
    "Venue:    ACM IUI (28th International Conference on Intelligent User Interfaces)\n",
    "Domain:   Human-AI Collaboration / Organisational Behaviour / HCI\n",
    "DOI:      https://doi.org/10.1145/3581641.3584052\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level of Operation\n",
    "\n",
    "```\n",
    "\u2610 Technical (algorithms, models, metrics)\n",
    "\u2611 Organisational (processes, roles, workflows)  \u2190 PRIMARY\n",
    "\u2610 Institutional (governance structures, accountability)\n",
    "\u2610 Societal (norms, public discourse, collective action)\n",
    "\u2610 Multi-level\n",
    "```\n",
    "\n",
    "**It exists because** `HUMAN BEHAVIOURAL RESPONSE TO AI-INITIATED DELEGATION` is currently under-specified or failing.\n",
    "\n",
    "> Existing delegation algorithms assume humans behave identically whether or not they know AI is managing their task allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Map\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  [Society / Regulation]                                     \u2502\n",
    "\u2502         \u2191                                                   \u2502\n",
    "\u2502         \u2502 (relationship: weak connection - not addressed)   \u2502\n",
    "\u2502         \u2502                                                   \u2502\n",
    "\u2502  [Organisation / Process]  \u2190 THIS PAPER INTERVENES HERE     \u2502\n",
    "\u2502         \u2191                                                   \u2502\n",
    "\u2502         \u2502 (relationship: AI as task allocator)              \u2502\n",
    "\u2502         \u2502                                                   \u2502\n",
    "\u2502  [AI System / Tool]                                         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Core Claim (Distilled)\n",
    "\n",
    "> ### The Core Conditional Claim\n",
    ">\n",
    "> **If** `[AI DELEGATION ALGORITHMS MATCH TASKS TO HUMAN CAPABILITIES]` holds,  \n",
    "> **then** `[DELEGATING INSTANCES WHERE HUMANS HAVE HIGHER PREDICTED CONFIDENCE]`  \n",
    "> **will lead to** `[IMPROVED TASK PERFORMANCE AND TASK SATISFACTION VIA INCREASED SELF-EFFICACY]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claim Assessment Scores\n",
    "claim_assessment = {\n",
    "    \"Confidence in assumption\": {\n",
    "        \"score\": \"Medium\",\n",
    "        \"rationale\": \"Tested in controlled lab setting with non-experts only\"\n",
    "    },\n",
    "    \"Mechanism specified\": {\n",
    "        \"score\": \"Yes\",\n",
    "        \"rationale\": \"Self-efficacy clearly identified as mediating variable\"\n",
    "    },\n",
    "    \"Outcome measurable\": {\n",
    "        \"score\": \"Yes\",\n",
    "        \"rationale\": \"Accuracy (objective), satisfaction/self-efficacy (Likert scales)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for criterion, details in claim_assessment.items():\n",
    "    print(f\"{criterion}: {details['score']}\")\n",
    "    print(f\"  \u2192 {details['rationale']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence Basis\n",
    "\n",
    "| Dimension | Assessment |\n",
    "|-----------|------------|\n",
    "| **Evidence type** | Empirical \u2014 Randomised controlled experiment |\n",
    "| **Evidence strength** | \ud83d\udfe2 **Strong** for controlled setting; \ud83d\udfe1 Moderate for generalisability |\n",
    "| **Sample/scope** | n=196 Prolific participants, image classification task, 3 conditions |\n",
    "| **Generalisability concerns** | Non-expert participants; generic task; short-duration study; no real stakes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Chain Diagram\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  [AI can accurately estimate human/AI confidence]              \u2502\n",
    "\u2502                          \u2193                                     \u2502\n",
    "\u2502               confidence: MEDIUM                               \u2502\n",
    "\u2502                          \u2193                                     \u2502\n",
    "\u2502  [Delegation to higher-confidence party]                       \u2502\n",
    "\u2502                          \u2193                                     \u2502\n",
    "\u2502               specified: YES                                   \u2502\n",
    "\u2502                          \u2193                                     \u2502\n",
    "\u2502  [Improved performance + satisfaction]                         \u2502\n",
    "\u2502                          \u2193                                     \u2502\n",
    "\u2502               measurable: YES                                  \u2502\n",
    "\u2502                          \u2193                                     \u2502\n",
    "\u2502  [Mechanism: Self-efficacy increase]                           \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. System Model\n",
    "\n",
    "### Implicit System\n",
    "\n",
    "**Pre-AI State:**\n",
    "```\n",
    "Human \u2192 Task Assignment (random/manager) \u2192 Task Execution \u2192 Outcome\n",
    "```\n",
    "\n",
    "**Post-AI State:**\n",
    "```\n",
    "Human \u2192 AI Delegation Model \u2192 Capability-Matched Tasks \u2192 Outcome\n",
    "           \u2191\n",
    "    Estimates both AI and\n",
    "    human confidence per instance\n",
    "```\n",
    "\n",
    "| Element | Status |\n",
    "|---------|--------|\n",
    "| **The gap is filled by** | AI model that estimates both AI and human confidence per instance |\n",
    "| **Accountability sits with** | *Unaddressed* \u2014 paper focuses on performance, not accountability |\n",
    "| **This is** | Implicit \u2014 system model exists but accountability unexamined |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Vector\n",
    "\n",
    "```\n",
    "\u2610 Tool substitution (AI replaces discrete task)\n",
    "\u2611 Process redesign (AI reshapes workflow)  \u2190 CLAIMED & DEMONSTRATED\n",
    "\u2610 Structural change (AI shifts roles/accountability)\n",
    "\u2610 Institutional evolution (AI changes governance norms)\n",
    "```\n",
    "\n",
    "**Assessment:** Paper is appropriately scoped \u2014 claims match specification level. \u2713"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Assumptions\n",
    "\n",
    "### 4.1 Technical Assumptions\n",
    "\n",
    "> The argument relies on technical stability around `AI CONFIDENCE ESTIMATION ACCURACY`,  \n",
    "> which is treated as: **Given** \u2014 the delegation algorithm's confidence estimates are assumed to correctly predict relative human/AI performance.\n",
    "\n",
    "**Critical technical dependencies:**\n",
    "1. Human predictions collected during training generalize to deployment\n",
    "2. Confidence calibration is accurate for both AI and human models\n",
    "3. Task characteristics (image classification) transfer to other domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Organisational Assumptions \u2b50 KEY SECTION\n",
    "\n",
    "**Organisational behaviour assumed:** \ud83d\udfe1 Optimistic / Unspecified\n",
    "\n",
    "| Actor | Assumption | Reality Check |\n",
    "|-------|------------|---------------|\n",
    "| **Leadership** | Not addressed | No org hierarchy in experimental setup |\n",
    "| **Middle management** | Not addressed | AI replaces managerial function |\n",
    "| **Operators/Users** | Accept AI task assignment without resistance | Lab setting eliminates power dynamics |\n",
    "| **Cross-functional coordination** | Not addressed | Isolated task context |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incentive Alignment Check\n",
    "incentive_check = {\n",
    "    \"Leadership\": {\n",
    "        \"optimises_for\": \"Efficiency, cost reduction\",\n",
    "        \"alignment\": \"?\",\n",
    "        \"status\": \"Unspecified\"\n",
    "    },\n",
    "    \"Middle Management\": {\n",
    "        \"optimises_for\": \"N/A \u2014 replaced by AI\",\n",
    "        \"alignment\": \"\u26a0\",\n",
    "        \"status\": \"Threatened\"\n",
    "    },\n",
    "    \"Operators/Workers\": {\n",
    "        \"optimises_for\": \"Task completion, satisfaction\",\n",
    "        \"alignment\": \"\u2713\",\n",
    "        \"status\": \"Aligned (per study)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"INCENTIVE ALIGNMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "for actor, details in incentive_check.items():\n",
    "    print(f\"\\n{actor}\")\n",
    "    print(f\"  Optimises for: {details['optimises_for']}\")\n",
    "    print(f\"  Alignment: {details['alignment']} {details['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Practice Gap\n",
    "\n",
    "| Dimension | Assessment |\n",
    "|-----------|------------|\n",
    "| **Policy assumes** | Algorithmic management can replicate/improve human management |\n",
    "| **Organisation actually does** | Unknown \u2014 study abstracts away org context |\n",
    "| **Gap explained by** | Lab setting eliminates org dynamics (power, politics, resource constraints) |\n",
    "| **Who benefits from gap** | Platform providers (Uber cited); researchers claiming clean results |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Policy/Institutional Assumptions \u2b50 KEY SECTION\n",
    "\n",
    "```\n",
    "This work assumes governance and accountability will:\n",
    "\u2610 Function as designed\n",
    "\u2610 Require adaptation (specifies how)\n",
    "\u2610 Face systematic barriers (identifies them)\n",
    "\u2611 Not addressed  \u2190 THIS PAPER\n",
    "```\n",
    "\n",
    "| Mechanism | Status |\n",
    "|-----------|--------|\n",
    "| **Enforcement mechanism** | Not addressed |\n",
    "| **Accountability mechanism** | Not addressed \u2014 critical gap for algorithmic management |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Lens Checks\n",
    "\n",
    "### 5.1 Human Behaviour Lens \u2014 Status: \ud83d\udfe2 EXPLICIT\n",
    "\n",
    "| Dimension | Assessment |\n",
    "|-----------|------------|\n",
    "| **Human role** | In-the-loop \u2014 humans execute delegated tasks |\n",
    "| **Attention/cognitive load** | \ud83d\udfe1 Partially addressed \u2014 cognitive ability measured as control variable |\n",
    "| **Trust calibration** | \ud83d\udfe1 Partially addressed \u2014 algorithm attitude measured |\n",
    "\n",
    "**Failure modes considered:**\n",
    "```\n",
    "\u2610 Automation bias (over-reliance on AI)\n",
    "\u2610 Algorithm aversion (under-reliance on AI)\n",
    "\u2610 Deskilling (capability atrophy)\n",
    "\u2610 Accountability diffusion\n",
    "\u2610 Alert fatigue\n",
    "\u2610 Expertise asymmetry\n",
    "\u2611 Self-efficacy effects \u2190 CENTRAL TO PAPER\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Incentives Lens \u2014 Status: \ud83d\udd34 ABSENT\n",
    "\n",
    "> The paper assumes actors optimise for `TASK COMPLETION AND SATISFACTION`,  \n",
    "> but does **NOT** account for strategic behaviour.\n",
    "\n",
    "| Risk | Assessment |\n",
    "|------|------------|\n",
    "| **Gaming potential** | \ud83d\udd34 Unaddressed \u2014 what if workers game confidence signals? |\n",
    "| **Goodhart risk** | \ud83d\udd34 Unaddressed \u2014 optimising for delegation may corrupt capability signals |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Institutional Reality Lens \u2014 Status: \ud83d\udd34 ABSENT\n",
    "\n",
    "```\n",
    "Institutional enforcement and accountability are:\n",
    "\u2610 Central to the paper's claims\n",
    "\u2610 Peripheral but acknowledged\n",
    "\u2610 Assumed to work\n",
    "\u2611 Irrelevant to scope  \u2190 THIS PAPER\n",
    "```\n",
    "\n",
    "| Dimension | Assessment |\n",
    "|-----------|------------|\n",
    "| **Cross-boundary coordination** | Not addressed |\n",
    "| **Regulatory capacity** | Not addressed |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparative Context\n",
    "\n",
    "| Dimension | This Paper |\n",
    "|-----------|------------|\n",
    "| **Stronger on** | Causal mechanism (self-efficacy); rigorous experimental design; mediating variable analysis |\n",
    "| **Weaker on** | Organisational context; institutional dynamics; long-term effects; expert domains |\n",
    "| **Novel contribution** | First to identify self-efficacy as mechanism; shows awareness of delegation doesn't change effect |\n",
    "| **Gap not addressed** | What happens when organisational incentives conflict with delegation logic? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Literature Positioning Matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# Create positioning chart\n",
    "papers = {\n",
    "    'This Paper\\n(Hemmer 2023)': (0.8, 0.2),\n",
    "    'M\u00f6hlmann &\\nHenfridsson 2019': (0.3, 0.7),\n",
    "    'Lee et al. 2015': (0.4, 0.6),\n",
    "    'Bondi et al. 2022': (0.7, 0.3),\n",
    "    'GAP': (0.8, 0.8)\n",
    "}\n",
    "\n",
    "colors = ['blue', 'green', 'green', 'blue', 'red']\n",
    "\n",
    "for (name, (x, y)), color in zip(papers.items(), colors):\n",
    "    ax.scatter(x, y, s=200, c=color, alpha=0.6, edgecolors='black')\n",
    "    ax.annotate(name, (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('Technical Depth', fontsize=12)\n",
    "ax.set_ylabel('Organisational/Policy Realism', fontsize=12)\n",
    "ax.set_title('Literature Positioning Matrix', fontsize=14)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(0.25, 0.95, 'Low Tech / High Org', ha='center', fontsize=8, alpha=0.5)\n",
    "ax.text(0.75, 0.95, 'High Tech / High Org\\n(TARGET)', ha='center', fontsize=8, alpha=0.5)\n",
    "ax.text(0.25, 0.05, 'Low Tech / Low Org', ha='center', fontsize=8, alpha=0.5)\n",
    "ax.text(0.75, 0.05, 'High Tech / Low Org', ha='center', fontsize=8, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Synthesis & Research Insight\n",
    "\n",
    "### Pattern Recognition\n",
    "\n",
    "> This paper **CONFIRMS** the pattern that:  \n",
    "> Human-AI collaboration research treats organisations as neutral transmission mechanisms \u2014 demonstrating effects in lab settings while acknowledging (but not addressing) that \"human behavior might deviate when teaming up with an AI model\" in real organisational contexts.\n",
    "\n",
    "**Recurring pattern across papers:**\n",
    "\n",
    "```\n",
    "ALGORITHMIC MANAGEMENT is framed as a TECHNICAL/BEHAVIOURAL issue,\n",
    "but consistently faces ORGANISATIONAL RESISTANCE and INSTITUTIONAL FRICTION \n",
    "that lab studies cannot capture.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution to Thesis\n",
    "\n",
    "| Dimension | Assessment |\n",
    "|-----------|------------|\n",
    "| \ud83d\udfe2 **Supports argument** | Demonstrates that even well-designed technical systems face \"unexamined\" organisational assumptions |\n",
    "| \ud83d\udfe1 **Challenges argument** | Shows positive effects possible when org context is controlled away \u2014 raises question of whether org friction is insurmountable |\n",
    "| **Provides evidence for** | Gap between lab-validated technical solutions and organisational implementation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary Conditions Analysis\n",
    "boundary_conditions = {\n",
    "    \"claims_hold_when\": [\n",
    "        \"Tasks are independent and parallelisable\",\n",
    "        \"Workers have no stake in task selection\",\n",
    "        \"No inter-worker dynamics or status hierarchies\",\n",
    "        \"Short time horizons (no deskilling)\",\n",
    "        \"No real consequences for errors\"\n",
    "    ],\n",
    "    \"claims_break_if\": [\n",
    "        \"Middle managers resist AI encroachment\",\n",
    "        \"Workers game confidence signals\",\n",
    "        \"Expert domains where identity tied to task range\",\n",
    "        \"Long-term deskilling concerns emerge\",\n",
    "        \"Regulatory requirements for human oversight\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"BOUNDARY CONDITIONS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\u2713 Claims HOLD when:\")\n",
    "for condition in boundary_conditions[\"claims_hold_when\"]:\n",
    "    print(f\"  \u2022 {condition}\")\n",
    "\n",
    "print(\"\\n\u2717 Claims BREAK if:\")\n",
    "for condition in boundary_conditions[\"claims_break_if\"]:\n",
    "    print(f\"  \u2022 {condition}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Boundary conditions acknowledged by authors: PARTIALLY\")\n",
    "print(\"  \u2192 Acknowledge non-expert task, short duration\")\n",
    "print(\"  \u2192 Suggest future work on expert domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Metadata & Linkages\n",
    "\n",
    "### Zotero Integration\n",
    "\n",
    "```yaml\n",
    "Tags: \n",
    "  - human-AI-collaboration\n",
    "  - AI-delegation\n",
    "  - algorithmic-management\n",
    "  - self-efficacy\n",
    "  - task-performance\n",
    "  - task-satisfaction\n",
    "  - experimental-study\n",
    "  - IUI\n",
    "  - org-assumptions-optimistic\n",
    "\n",
    "Collections:\n",
    "  - Core-Human-Machine\n",
    "  - Algorithmic-Management\n",
    "  - Org-Theory-Gap\n",
    "\n",
    "Related Items:\n",
    "  - bondi2022role\n",
    "  - fugener2022cognitive\n",
    "  - lee2015working\n",
    "  - mohlmann2019uber\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Questions Relevance\n",
    "\n",
    "| Research Question | Relevance |\n",
    "|-------------------|----------|\n",
    "| RQ1: How do orgs implement AI governance? | Medium (3/5) |\n",
    "| RQ2: Why do AI implementations face org friction? | \ud83d\udfe2 **High (5/5)** |\n",
    "| RQ3: What institutional factors mediate AI adoption? | Low (2/5) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-ups\n",
    "\n",
    "**Questions to revisit:**\n",
    "- What happens to self-efficacy effects in high-stakes expert domains?\n",
    "- How do middle managers respond when AI takes over task allocation?\n",
    "- Do long-term deskilling effects erode initial performance gains?\n",
    "\n",
    "**Connections to other papers:**\n",
    "- Contrast with M\u00f6hlmann & Henfridsson (2019) on Uber driver negative perceptions of algorithmic management\n",
    "- Compare with Lee et al. (2015) on worker reactions to algorithmic vs. human managers\n",
    "\n",
    "**Potential quotes for thesis:**\n",
    "- Page 2: *\"Human behavior might deviate when teaming up with an AI model\"* \u2014 acknowledges gap but doesn't address\n",
    "- Page 9: *\"Managers could consider applying AI delegation to yield higher levels of performance\"* \u2014 optimistic framing\n",
    "- Page 9: *\"Modified nature of the task through AI delegation was responsible for the increases\"* \u2014 key finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Findings Summary\n",
    "\n",
    "### Main Results (from paper)\n",
    "\n",
    "| Measure | Delegation Group | Human-Alone | Significance |\n",
    "|---------|-----------------|-------------|-------------|\n",
    "| Task Performance | 84.51% | 67.13% | p < 0.001 |\n",
    "| Task Satisfaction | 3.65 | 3.35 | p < 0.01 |\n",
    "| Self-Efficacy | 5.29 | 4.63 | p < 0.001 |\n",
    "\n",
    "**Key insight:** No significant difference between \"aware\" and \"hidden\" delegation groups \u2014 suggesting the task modification (not awareness) drives effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise main findings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "categories = ['Task\\nPerformance\\n(%)', 'Task\\nSatisfaction\\n(1-5)', 'Self-Efficacy\\n(1-7)']\n",
    "delegation = [84.51, 3.65, 5.29]\n",
    "human_alone = [67.13, 3.35, 4.63]\n",
    "\n",
    "# Normalize for comparison\n",
    "delegation_norm = [84.51, 3.65*20, 5.29*14.3]  # Scale to ~similar range for visualisation\n",
    "human_alone_norm = [67.13, 3.35*20, 4.63*14.3]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, delegation, width, label='AI Delegation', color='#27AE60', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, human_alone, width, label='Human Alone', color='#3498DB', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Key Findings: AI Delegation vs Human Alone', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.legend()\n",
    "\n",
    "# Add significance markers\n",
    "ax.annotate('***', xy=(0, 87), ha='center', fontsize=12)\n",
    "ax.annotate('**', xy=(1, 3.8), ha='center', fontsize=12)\n",
    "ax.annotate('***', xy=(2, 5.5), ha='center', fontsize=12)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, delegation):\n",
    "    ax.annotate(f'{val:.2f}' if val < 10 else f'{val:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar, val in zip(bars2, human_alone):\n",
    "    ax.annotate(f'{val:.2f}' if val < 10 else f'{val:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSignificance levels: *** p < 0.001, ** p < 0.01, * p < 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Thesis Relevance Summary\n",
    "\n",
    "### How this paper supports the thesis argument:\n",
    "\n",
    "> **Thesis:** *\"AI governance literature treats organisations as transmission mechanisms for policy intent, but organisations are actually sites of contestation, adaptation, and goal displacement.\"*\n",
    "\n",
    "This paper **indirectly supports** the thesis by:\n",
    "\n",
    "1. **Demonstrating the gap** between controlled lab findings and organisational reality\n",
    "2. **Acknowledging but not addressing** organisational dynamics (p.2: \"human behavior might deviate\")\n",
    "3. **Implicitly assuming** organisational acceptance of algorithmic management\n",
    "4. **Citing Uber** as an example \u2014 a case where organisational/worker dynamics proved contentious\n",
    "\n",
    "### Critical questions this paper raises for the thesis:\n",
    "\n",
    "1. If delegation improves performance even when hidden, does awareness of AI management matter?\n",
    "2. Can the self-efficacy mechanism survive organisational politics?\n",
    "3. What happens when middle managers are threatened by AI task allocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotation summary\n",
    "summary = \"\"\"\n",
    "PAPER ANNOTATION SUMMARY\n",
    "========================\n",
    "Title: Human-AI Collaboration: The Effect of AI Delegation on Human \n",
    "       Task Performance and Task Satisfaction\n",
    "Authors: Hemmer et al. (2023)\n",
    "Venue: ACM IUI\n",
    "\n",
    "CLASSIFICATION\n",
    "  Paper Type: Core\n",
    "  Relevance: 5/5\n",
    "  Bucket: Human-Machine Interaction\n",
    "\n",
    "CORE CLAIM\n",
    "  If AI matches tasks to human capabilities,\n",
    "  Then delegation improves performance/satisfaction\n",
    "  Via increased self-efficacy (mediating mechanism)\n",
    "\n",
    "KEY STRENGTHS\n",
    "  \u2022 Rigorous experimental design (n=196)\n",
    "  \u2022 Clear mechanism identification (self-efficacy)\n",
    "  \u2022 Novel finding: awareness doesn't change effect\n",
    "\n",
    "KEY LIMITATIONS FOR THESIS\n",
    "  \u2022 Org context abstracted away\n",
    "  \u2022 Incentive dynamics unexamined\n",
    "  \u2022 Institutional/accountability absent\n",
    "  \u2022 Non-expert, short-term, no real stakes\n",
    "\n",
    "THESIS CONTRIBUTION\n",
    "  Supports: Gap between lab validation and org implementation\n",
    "  Challenges: Positive effects when org friction removed\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}